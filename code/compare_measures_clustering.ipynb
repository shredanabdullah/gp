{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TwwiSN8p8oR",
        "outputId": "8cb354b2-ded0-46ab-e43d-0d7d90b2d4ab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top K Code Files:\n",
            "path/to/website1\n",
            "path/to/website4\n",
            "path/to/website5\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import gensim.downloader as api\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return tokens\n",
        "\n",
        "def preprocess_texts(texts):\n",
        "    return [preprocess_text(text) for text in texts]\n",
        "\n",
        "\n",
        "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "def text_to_embedding(texts, model, tfidf_vectorizer):\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform([' '.join(text) for text in texts])\n",
        "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "    def get_word_vector(word):\n",
        "        try:\n",
        "            return model[word]\n",
        "        except KeyError:\n",
        "            return np.zeros(model.vector_size)\n",
        "\n",
        "    embeddings = []\n",
        "    for text in texts:\n",
        "        text_embedding = np.zeros(model.vector_size)\n",
        "        word_count = 0\n",
        "        for word in text:\n",
        "            if word in feature_names:\n",
        "                tfidf_value = tfidf_vectorizer.transform([' '.join([word])]).data[0]\n",
        "                word_embedding = get_word_vector(word) * tfidf_value\n",
        "                text_embedding += word_embedding\n",
        "                word_count += 1\n",
        "        if word_count > 0:\n",
        "            text_embedding /= word_count\n",
        "        embeddings.append(text_embedding)\n",
        "    return np.array(embeddings)\n",
        "\n",
        "def cluster_descriptions(embeddings, num_clusters=2):\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
        "    clusters = kmeans.fit_predict(embeddings)\n",
        "    return kmeans, clusters\n",
        "\n",
        "def classify_user_input(user_input, word2vec_model, tfidf_vectorizer, kmeans):\n",
        "    preprocessed_input = preprocess_text(user_input)\n",
        "    input_embedding = text_to_embedding([preprocessed_input], word2vec_model, tfidf_vectorizer)[0]\n",
        "    cluster = kmeans.predict([input_embedding])[0]\n",
        "    return cluster\n",
        "\n",
        "def compute_similarity(embeddings, query_embedding):\n",
        "    similarities = cosine_similarity([query_embedding], embeddings).flatten()\n",
        "    return similarities\n",
        "\n",
        "def retrieve_top_k_websites(user_input, descriptions, description_to_code_files, word2vec_model, tfidf_vectorizer, kmeans, clusters, k=5):\n",
        "    preprocessed_descriptions = preprocess_texts(descriptions)\n",
        "    embeddings = text_to_embedding(preprocessed_descriptions, word2vec_model, tfidf_vectorizer)\n",
        "\n",
        "    design_descriptions = [descriptions[i] for i in range(len(clusters)) if clusters[i] == 0]\n",
        "    content_descriptions = [descriptions[i] for i in range(len(clusters)) if clusters[i] == 1]\n",
        "\n",
        "    user_cluster = classify_user_input(user_input, word2vec_model, tfidf_vectorizer, kmeans)\n",
        "\n",
        "    if user_cluster == 0:\n",
        "        relevant_descriptions = design_descriptions\n",
        "    else:\n",
        "        relevant_descriptions = content_descriptions\n",
        "\n",
        "    preprocessed_input = preprocess_text(user_input)\n",
        "    input_embedding = text_to_embedding([preprocessed_input], word2vec_model, tfidf_vectorizer)[0]\n",
        "    similarities = compute_similarity(embeddings, input_embedding)\n",
        "\n",
        "    top_k_indices = similarities.argsort()[-k:][::-1]\n",
        "    top_k_code_files = [description_to_code_files[descriptions[i]] for i in top_k_indices if descriptions[i] in relevant_descriptions]\n",
        "\n",
        "    return top_k_code_files[:k]\n",
        "\n",
        "descriptions = [\n",
        "    \"This website is an e-commerce platform for selling computers. It has a modern design with a dark theme.\",\n",
        "    \"A blog about travel and adventure experiences around the world. The design features vibrant colors and a responsive layout.\",\n",
        "    \"A portfolio website showcasing graphic design and photography work. It uses a minimalist design with a focus on visual content.\",\n",
        "    \"An online platform for learning and practicing programming languages. The website has a clean design with easy navigation and code examples.\",\n",
        "    \"A news website providing the latest updates on technology and science. It has a professional design with a structured layout and news sections.\"\n",
        "]\n",
        "\n",
        "code_file_paths = [\n",
        "    \"path/to/website1\",\n",
        "    \"path/to/website2\",\n",
        "    \"path/to/website3\",\n",
        "    \"path/to/website4\",\n",
        "    \"path/to/website5\"\n",
        "]\n",
        "\n",
        "description_to_code_files = {desc: path for desc, path in zip(descriptions, code_file_paths)}\n",
        "\n",
        "preprocessed_descriptions = preprocess_texts(descriptions)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "embeddings = text_to_embedding(preprocessed_descriptions, word2vec_model, tfidf_vectorizer)\n",
        "\n",
        "kmeans, clusters = cluster_descriptions(embeddings)\n",
        "\n",
        "user_input = \"A platform to buy the latest gadgets and electronic devices. It should have a modern design with interactive elements.\"\n",
        "\n",
        "top_k_code_files = retrieve_top_k_websites(user_input, descriptions, description_to_code_files, word2vec_model, tfidf_vectorizer, kmeans, clusters, k=3)\n",
        "\n",
        "print(\"Top K Code Files:\")\n",
        "for code_file in top_k_code_files:\n",
        "    print(code_file)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
